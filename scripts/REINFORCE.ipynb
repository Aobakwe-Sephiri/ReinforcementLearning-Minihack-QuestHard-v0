{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import minihack\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Improvements to model/algorithm (maybe just include baseline? Or change to actor-critic)\n",
    "1. Hyperparameter Tuning\n",
    "\n",
    "1 colab notebook per experiment, with a folder dedicated to that experiment. 5 of each in said folder:\n",
    "1. saved policy model\n",
    "1. saved gif (2: best gif, last gif)\n",
    "1. saved scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful URLs\n",
    "\n",
    "https://minihack.readthedocs.io/en/latest/index.html\n",
    "\n",
    "https://minihack.readthedocs.io/en/latest/getting-started/observation_spaces.html#options\n",
    "\n",
    "https://discuss.pytorch.org/t/pytorch-multiple-input-and-output/140020/2\n",
    "\n",
    "https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/12/REINFORCE-CartPole.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapes(observation):\n",
    "    '''\n",
    "    Args: \n",
    "        observation: (dict) An environment observation\n",
    "    Ret:\n",
    "        shapes: (list) A list of the shapes of the numpy arrays in the observation dict\n",
    "    '''\n",
    "    shapes = []\n",
    "    for key in observation.keys():\n",
    "        if not key == \"pixel\":\n",
    "            thing = observation[key]\n",
    "            shapes.append(thing.shape)\n",
    "    return shapes\n",
    "\n",
    "def save_gif(gif,path=None):\n",
    "    '''\n",
    "    Args:\n",
    "        gif: a list of image objects\n",
    "        path: the path to save the gif to, defaults to <current datetime>.gif\n",
    "    '''\n",
    "    if path is None:\n",
    "        path = '/home/evan/RL_Assignment/repo/video'+datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") + '.gif'\n",
    "    gif[0].save(path, save_all=True,optimize=False, append_images=gif[1:], loop=0)\n",
    "    print(\"Saved Video\")\n",
    "\n",
    "def save_policy_model(policy_model,PATH):\n",
    "    torch.save(policy_model.state_dict(), PATH)\n",
    "\n",
    "def load_policy_model(model,PATH):\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    return model\n",
    "\n",
    "def save_scores(scores,PATH):\n",
    "    array = np.array(scores)\n",
    "    np.savetxt(PATH+\"scores.txt\", array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "# obs_keys = ('glyphs', 'chars','screen_descriptions', 'glyphs_crop', 'chars_crop', 'screen_descriptions_crop','blstats', 'message', 'inv_strs', 'pixel')\n",
    "obs_keys = ('glyphs','glyphs_crop','inv_glyphs', 'pixel')\n",
    "\n",
    "env = gym.make(\n",
    "    \"MiniHack-Quest-Hard-v0\",\n",
    "    observation_keys=obs_keys\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropPolicy(nn.Module):\n",
    "    def __init__(self, env, hidden_size=100):\n",
    "        super(CropPolicy, self).__init__()\n",
    "\n",
    "        # initialising state\n",
    "        state = env.reset()\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Getting the shapes of the state space\n",
    "        shapes = get_shapes(state)\n",
    "        self.obs_keys = obs_keys\n",
    "\n",
    "        input_size = 136 # self.get_input_shape(shapes)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def get_input_shape(self,shapes):\n",
    "        length = 0\n",
    "        for shape in shapes:\n",
    "            length +=sum(shape)\n",
    "        return length\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action probabilites\n",
    "\n",
    "        '''\n",
    "        \n",
    "        glyph_features = torch.from_numpy(state['glyphs_crop']).float().to(device)\n",
    "        glyph_features = torch.flatten(glyph_features)\n",
    "        inv_features = torch.from_numpy(state['inv_glyphs']).to(device)\n",
    "        x = torch.cat((glyph_features.float(),inv_features.float()))\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        action_probabilities = torch.tanh(self.fc2(x)).float()\n",
    "        # print(action_probabilities)\n",
    "        # we just consider 1 dimensional probability of action\n",
    "        return F.softmax(action_probabilities,dim=0)\n",
    "    \n",
    "    def act(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action,probability(action|policy,state)\n",
    "\n",
    "        '''\n",
    "        probs = self.forward(state).cpu()\n",
    "        # print(probs)\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullViewPolicy(nn.Module):\n",
    "    def __init__(self, env, hidden_size=100):\n",
    "        super(FullViewPolicy, self).__init__()\n",
    "\n",
    "        # initialising state\n",
    "        state = env.reset()\n",
    "        self.glyphState = np.array([\n",
    "            state['glyphs'],\n",
    "            state['glyphs'],\n",
    "            state['glyphs']\n",
    "        ])\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Getting the shapes of the state space\n",
    "        shapes = get_shapes(state)\n",
    "        self.obs_keys = obs_keys\n",
    "\n",
    "        self.glyphConv2D1 = nn.Conv2d(3,1,(3,5),stride=(2,3))\n",
    "        self.glyphMaxPool1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(115, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action probabilites\n",
    "\n",
    "        '''\n",
    "        self.glyphState = np.insert(self.glyphState[:-1,:,:],0,state['glyphs'],axis=0)\n",
    "        \n",
    "        glyph_features = torch.from_numpy(self.glyphState ).float().unsqueeze(0).to(device)\n",
    "        glyph_features = F.relu(self.glyphConv2D1(glyph_features))\n",
    "        glyph_features = torch.flatten(self.glyphMaxPool1(glyph_features))\n",
    "        inv_features = torch.from_numpy(state['inv_glyphs']).to(device)\n",
    "        x = torch.cat((glyph_features.float(),inv_features.float()))\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        action_probabilites = torch.tanh(self.fc3(x))\n",
    "        # we just consider 1 dimensional probability of action\n",
    "        return F.softmax(action_probabilites, dim=-1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action,probability(action|policy,state)\n",
    "\n",
    "        '''\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.dot(rewards,np.power(np.ones(len(rewards))*gamma,np.arange(len(rewards))))\n",
    "    raise NotImplementedError\n",
    "    return returns\n",
    "\n",
    "\n",
    "def compute_returns_naive_baseline(rewards, gamma):\n",
    "    # raise NotImplementedError\n",
    "    mean = np.mean(rewards)\n",
    "    std = np.std(rewards)\n",
    "    returns =  np.multiply(rewards,np.power(np.ones(len(rewards))*gamma,np.arange(len(rewards))))\n",
    "    \n",
    "\n",
    "    return returns,mean,std# (sum(returns)-mean)/std\n",
    "\n",
    "\n",
    "def reinforce(policy, optimizer, seed, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    '''\n",
    "    Args:\n",
    "        policy: a pytorch model, takes in a state and outputs an action\n",
    "        optimizer: a pytorch optimizer\n",
    "        n_episodes: (int) number of episodes to train for\n",
    "        max_t: (int) max time steps per episode\n",
    "        gamma: (float, [0,1]) discount factor\n",
    "        print_every: (int) number of episodes between print of update\n",
    "    \n",
    "    Ret:\n",
    "        scores: list of total rewards per episode\n",
    "        gif: a list of images corresponding to the best scoring episode\n",
    "    '''\n",
    "    print(\"Starting Reinforce\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    best_frames = None\n",
    "    latest_frames=None\n",
    "    best_score = None\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        frames = []\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            # action = np.random.randint(0, high=85)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action) \n",
    "            frame = state[\"pixel\"]\n",
    "            frames.append(frame)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        score = sum(rewards)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Keep track of the gif of the best scoring episode\n",
    "        latest_frames = frames\n",
    "        if best_score is None or score > best_score: \n",
    "            best_score=score\n",
    "            best_frames = frames\n",
    "        \n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        R = sum([a * b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        # Calculate the loss \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        # print(policy_loss[0])\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "    gif = []\n",
    "    for image in best_frames:\n",
    "        gif.append(Image.fromarray(image, \"RGB\"))\n",
    "    last_gif =[]\n",
    "    for image in latest_frames:\n",
    "        last_gif.append(Image.fromarray(image, \"RGB\"))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return scores,gif,last_gif\n",
    "\n",
    "\n",
    "def reinforce_naive_baseline(policy, optimizer, seed,n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    '''\n",
    "    Args:\n",
    "        policy: a pytorch model, takes in a state and outputs an action\n",
    "        optimizer: a pytorch optimizer\n",
    "        n_episodes: (int) number of episodes to train for\n",
    "        max_t: (int) max time steps per episode\n",
    "        gamma: (float, [0,1]) discount factor\n",
    "        print_every: (int) number of episodes between print of update\n",
    "    \n",
    "    Ret:\n",
    "        scores: list of total rewards per episode\n",
    "        gif: a list of images corresponding to the best scoring episode\n",
    "    '''\n",
    "    print(\"Starting Reinforce\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.seed(seed)\n",
    "\n",
    "    best_frames = None\n",
    "    latest_frames=None\n",
    "    best_score = None\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        frames = []\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            # action = np.random.randint(0, high=85)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action) \n",
    "            frame = state[\"pixel\"]\n",
    "            frames.append(frame)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        score = sum(rewards)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Keep track of the gif of the best scoring episode\n",
    "        latest_frames = frames\n",
    "        if best_score is None or score > best_score: \n",
    "            best_score=score\n",
    "            best_frames = frames\n",
    "        \n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        # discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        G,G_mean,G_std = compute_returns_naive_baseline(rewards, gamma)\n",
    "\n",
    "        # R = sum([a * b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        # Calculate the loss \n",
    "        policy_loss = []\n",
    "        for i in range(len(saved_log_probs)):\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            log_prob = saved_log_probs[i]\n",
    "            denominator = max(G_std,1e-2) \n",
    "            policy_loss.append(-log_prob * (sum(G[i:])-G_mean)/denominator)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        # print(policy_loss[0])\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "    gif = []\n",
    "    for image in best_frames:\n",
    "        gif.append(Image.fromarray(image, \"RGB\"))\n",
    "    last_gif =[]\n",
    "    for image in latest_frames:\n",
    "        last_gif.append(Image.fromarray(image, \"RGB\"))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return scores,gif,last_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seeds = np.random.randint(1000, size=5)\n",
    "iteration = 0\n",
    "for seed in seeds:\n",
    "    policy = CropPolicy(env).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    scores,gif,last_gif = reinforce(policy, optimizer,int(seed), n_episodes=1000,max_t=10000,print_every=5)\n",
    "    PATH = str(iteration)+'.pth' # TODO path\n",
    "    save_policy_model(policy,PATH) # TODO path\n",
    "    save_gif(gif) # TODO path\n",
    "    save_gif(last_gif) # TODO path\n",
    "    save_scores(scores,PATH) # TODO path\n",
    "    iteration+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seeds = np.random.randint(1000, size=5)\n",
    "iteration = 0\n",
    "for seed in seeds:\n",
    "    policy = CropPolicy(env).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    scores,gif,last_gif = reinforce_naive_baseline(policy, optimizer,int(seed), n_episodes=1000,max_t=10000,print_every=5)\n",
    "    PATH = str(iteration)+'.pth' # TODO path\n",
    "    save_policy_model(policy,PATH) # TODO path\n",
    "    save_gif(gif) # TODO path\n",
    "    save_gif(last_gif) # TODO path\n",
    "    save_scores(scores,PATH) # TODO path\n",
    "    iteration+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seeds = np.random.randint(1000, size=5)\n",
    "iteration = 0\n",
    "for seed in seeds:\n",
    "    policy = FullViewPolicy(env).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "    scores,gif,last_gif = reinforce_naive_baseline(policy, optimizer,int(seed), n_episodes=1000,max_t=10000,print_every=5)\n",
    "    PATH = str(iteration)+'.pth' # TODO path\n",
    "    save_policy_model(policy,PATH) # TODO path\n",
    "    save_gif(gif) # TODO path\n",
    "    save_gif(last_gif) # TODO path\n",
    "    save_scores(scores,PATH) # TODO path\n",
    "    iteration+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Video\n",
      "Saved Video\n"
     ]
    }
   ],
   "source": [
    "save_gif(gif)\n",
    "save_gif(last_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/evan/RL_Assignment/repo')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "pathlib.Path().resolve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nle': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d61d25e1d05acdfa46420ff41b28ca1a60bdfb91fc85449c48770d29c2d77610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
