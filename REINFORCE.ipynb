{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/anaconda3/envs/nle/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import minihack\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Figure out why its not learning (could be because we are using the whole glyphs, and not just glyphs_crop)\n",
    "1. Figure out how to save and load policy object\n",
    "1. Figure out how to run on a Cluster or on Colab\n",
    "1. Improvements to model/algorithm (maybe just include baseline? Or change to actor-critic)\n",
    "1. Hyperparameter Tuning\n",
    "1. Set up to run with multiple seeds for statistical significance purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful URLs\n",
    "\n",
    "https://minihack.readthedocs.io/en/latest/index.html\n",
    "\n",
    "https://minihack.readthedocs.io/en/latest/getting-started/observation_spaces.html#options\n",
    "\n",
    "https://discuss.pytorch.org/t/pytorch-multiple-input-and-output/140020/2\n",
    "\n",
    "https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/12/REINFORCE-CartPole.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapes(observation):\n",
    "    '''\n",
    "    Args: \n",
    "        observation: (dict) An environment observation\n",
    "    Ret:\n",
    "        shapes: (list) A list of the shapes of the numpy arrays in the observation dict\n",
    "    '''\n",
    "    shapes = []\n",
    "    for key in observation.keys():\n",
    "        if not key == \"pixel\":\n",
    "            thing = observation[key]\n",
    "            shapes.append(thing.shape)\n",
    "    return shapes\n",
    "\n",
    "def save_gif(gif,path=None):\n",
    "    '''\n",
    "    Args:\n",
    "        gif: a list of image objects\n",
    "        path: the path to save the gif to, defaults to <current datetime>.gif\n",
    "    '''\n",
    "    if path is None:\n",
    "        path = '/home/evan/RL_Assignment/repo/video'+datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") + '.gif'\n",
    "    gif[0].save(path, save_all=True,optimize=False, append_images=gif[1:], loop=0)\n",
    "    print(\"Saved Video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/evan/RL_Assignment/ttyrec_files\"\n",
    "device = torch.device(\"cpu\")\n",
    "# obs_keys = ('glyphs', 'chars','screen_descriptions', 'glyphs_crop', 'chars_crop', 'screen_descriptions_crop','blstats', 'message', 'inv_strs', 'pixel')\n",
    "obs_keys = ('glyphs_crop','inv_glyphs', 'pixel')\n",
    "\n",
    "env = gym.make(\n",
    "    \"MiniHack-Quest-Hard-v0\",\n",
    "    savedir=save_dir,\n",
    "    observation_keys=obs_keys\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, env, hidden_size=100):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # initialising state\n",
    "        state = env.reset()\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Getting the shapes of the state space\n",
    "        shapes = get_shapes(state)\n",
    "        self.obs_keys = obs_keys\n",
    "\n",
    "        input_size = 136 # self.get_input_shape(shapes)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def get_input_shape(self,shapes):\n",
    "        length = 0\n",
    "        for shape in shapes:\n",
    "            length +=sum(shape)\n",
    "        return length\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action probabilites\n",
    "\n",
    "        '''\n",
    "        \n",
    "        glyph_features = torch.from_numpy(state['glyphs_crop']).float().to(device)\n",
    "        glyph_features = torch.flatten(glyph_features)\n",
    "        inv_features = torch.from_numpy(state['inv_glyphs']).to(device)\n",
    "        x = torch.cat((glyph_features.float(),inv_features.float()))\n",
    "        \n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        action_probabilities = torch.tanh(self.fc2(x)).float()\n",
    "        # print(action_probabilities)\n",
    "        # we just consider 1 dimensional probability of action\n",
    "        return F.softmax(action_probabilities,dim=0)\n",
    "    \n",
    "    def act(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action,probability(action|policy,state)\n",
    "\n",
    "        '''\n",
    "        probs = self.forward(state).cpu()\n",
    "        # print(probs)\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Policy(nn.Module):\n",
    "#     def __init__(self, env, hidden_size=100):\n",
    "#         super(Policy, self).__init__()\n",
    "\n",
    "#         # initialising state\n",
    "#         state = env.reset()\n",
    "#         self.glyphState = np.array([\n",
    "#             state['glyphs'],\n",
    "#             state['glyphs'],\n",
    "#             state['glyphs']\n",
    "#         ])\n",
    "#         action_size = env.action_space.n\n",
    "\n",
    "#         # Getting the shapes of the state space\n",
    "#         shapes = get_shapes(state)\n",
    "#         self.obs_keys = obs_keys\n",
    "\n",
    "#         self.glyphConv2D1 = nn.Conv2d(3,1,(3,5),stride=(2,3))\n",
    "#         self.glyphMaxPool1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "#         self.fc1 = nn.Linear(115, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         '''\n",
    "#         Args: state, an observation from env.step(). \n",
    "#             state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "#         Ret: action probabilites\n",
    "\n",
    "#         '''\n",
    "#         self.glyphState = np.insert(self.glyphState[:-1,:,:],0,state['glyphs'],axis=0)\n",
    "        \n",
    "#         glyph_features = torch.from_numpy(self.glyphState ).float().unsqueeze(0).to(device)\n",
    "#         glyph_features = F.relu(self.glyphConv2D1(glyph_features))\n",
    "#         glyph_features = torch.flatten(self.glyphMaxPool1(glyph_features))\n",
    "#         inv_features = torch.from_numpy(state['inv_glyphs']).to(device)\n",
    "#         x = torch.cat((glyph_features.float(),inv_features.float()))\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         action_probabilites = F.relu(self.fc2(x))\n",
    "#         # we just consider 1 dimensional probability of action\n",
    "#         return F.softmax(action_probabilites, dim=-1)\n",
    "    \n",
    "#     def act(self, state):\n",
    "#         '''\n",
    "#         Args: state, an observation from env.step(). \n",
    "#             state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "#         Ret: action,probability(action|policy,state)\n",
    "\n",
    "#         '''\n",
    "#         probs = self.forward(state).cpu()\n",
    "#         model = Categorical(probs)\n",
    "#         action = model.sample()\n",
    "#         return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    '''\n",
    "    Args:\n",
    "        policy: a pytorch model, takes in a state and outputs an action\n",
    "        optimizer: a pytorch optimizer\n",
    "        n_episodes: (int) number of episodes to train for\n",
    "        max_t: (int) max time steps per episode\n",
    "        gamma: (float, [0,1]) discount factor\n",
    "        print_every: (int) number of episodes between print of update\n",
    "    \n",
    "    Ret:\n",
    "        scores: list of total rewards per episode\n",
    "        gif: a list of images corresponding to the best scoring episode\n",
    "    '''\n",
    "    print(\"Starting Reinforce\")\n",
    "    best_frames = None\n",
    "    latest_frames=None\n",
    "    best_score = None\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        frames = []\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            # action = np.random.randint(0, high=85)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action) \n",
    "            frame = state[\"pixel\"]\n",
    "            frames.append(frame)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        score = sum(rewards)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Keep track of the gif of the best scoring episode\n",
    "        latest_frames = frames\n",
    "        if best_score is None or score > best_score: \n",
    "            best_score=score\n",
    "            best_frames = frames\n",
    "        \n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        R = sum([a * b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        # Calculate the loss \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        # print(policy_loss[0])\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "    gif = []\n",
    "    for image in best_frames:\n",
    "        gif.append(Image.fromarray(image, \"RGB\"))\n",
    "    last_gif =[]\n",
    "    for image in latest_frames:\n",
    "        last_gif.append(Image.fromarray(image, \"RGB\"))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return scores,gif,last_gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(env).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1013,  0.9376,  0.1056, -0.5770])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([ 4.8951e+02,  1.8116e+03,  3.6915e+02, -2.8129e+01,  2.7978e+03]).float()#,\n",
    "        # -5.9301e+01,  5.2306e+02,  4.5036e+02,  8.7317e+02,  1.1032e+03,\n",
    "        # -2.1825e+02, -8.3054e+01, -1.0596e+02,  9.2776e+02,  7.9150e+02,\n",
    "        #  1.4746e+03, -3.9986e+00, -1.1751e+02, -4.8166e+01,  8.0139e+02,\n",
    "        #  5.5373e+02, -2.2087e+02, -1.6877e+01, -2.5839e+02,  8.8373e+02,\n",
    "        # -6.3782e+00,  4.9773e+02, -7.6848e+00, -3.6406e+02,  1.0373e+03,\n",
    "        # -3.5552e+01, -1.8686e+02, -2.4091e+01,  3.3224e+02, -1.0081e+02,\n",
    "        # -1.2646e+02,  1.8410e+03, -2.8760e+02, -5.7098e+01,  9.4553e+02,\n",
    "        # -1.7202e+01,  8.1378e+02, -8.1557e+01, -4.2418e+02, -2.4669e+01,\n",
    "        #  2.8906e+02, -3.5567e+02, -3.2394e+02, -2.1453e+02,  1.5906e+03,\n",
    "        # -1.3053e+02,  8.4127e+02, -1.8754e+02,  5.1593e+02,  7.5083e+02,\n",
    "        # -4.6466e+02,  9.2046e-01,  6.9515e+02,  5.9445e+02,  6.8192e+02,\n",
    "        # -2.3958e+02,  6.1424e+02,  6.8675e+02, -3.9655e+01, -3.1689e+01,\n",
    "        # -4.3349e+01,  5.1774e+02,  6.2190e+02,  1.1146e+02, -1.7059e+02,\n",
    "        #  1.8702e+02, -3.5262e+01, -1.2733e+02,  1.4611e+03, -9.3537e+01,\n",
    "        # -1.2122e+02,  1.2088e+03, -2.3834e+02,  3.3737e+02,  3.6786e+02,\n",
    "        #  1.1546e+03,  3.2439e+02,  8.0422e+02,  9.0449e+01,  1.5224e+03]).float()\n",
    "\n",
    "x = torch.randn(4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369.15, 2797.8, 1522.4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(3.6915e+02),float(2.7978e+03),float(1.5224e+03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.tensor([ float(3.6915e+02),float(2.7978e+03),float(1.5224e+03)]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.8951e+02,  1.8116e+03,  3.6915e+02, -2.8129e+01,  2.7978e+03,\n",
      "        -5.9301e+01,  5.2306e+02,  4.5036e+02,  8.7317e+02,  1.1032e+03,\n",
      "        -2.1825e+02, -8.3054e+01, -1.0596e+02,  9.2776e+02,  7.9150e+02,\n",
      "         1.4746e+03, -3.9986e+00, -1.1751e+02, -4.8166e+01,  8.0139e+02,\n",
      "         5.5373e+02, -2.2087e+02, -1.6877e+01, -2.5839e+02,  8.8373e+02,\n",
      "        -6.3782e+00,  4.9773e+02, -7.6848e+00, -3.6406e+02,  1.0373e+03,\n",
      "        -3.5552e+01, -1.8686e+02, -2.4091e+01,  3.3224e+02, -1.0081e+02,\n",
      "        -1.2646e+02,  1.8410e+03, -2.8760e+02, -5.7098e+01,  9.4553e+02,\n",
      "        -1.7202e+01,  8.1378e+02, -8.1557e+01, -4.2418e+02, -2.4669e+01,\n",
      "         2.8906e+02, -3.5567e+02, -3.2394e+02, -2.1453e+02,  1.5906e+03,\n",
      "        -1.3053e+02,  8.4127e+02, -1.8754e+02,  5.1593e+02,  7.5083e+02,\n",
      "        -4.6466e+02,  9.2046e-01,  6.9515e+02,  5.9445e+02,  6.8192e+02,\n",
      "        -2.3958e+02,  6.1424e+02,  6.8675e+02, -3.9655e+01, -3.1689e+01,\n",
      "        -4.3349e+01,  5.1774e+02,  6.2190e+02,  1.1146e+02, -1.7059e+02,\n",
      "         1.8702e+02, -3.5262e+01, -1.2733e+02,  1.4611e+03, -9.3537e+01,\n",
      "        -1.2122e+02,  1.2088e+03, -2.3834e+02,  3.3737e+02,  3.6786e+02,\n",
      "         1.1546e+03,  3.2439e+02,  8.0422e+02,  9.0449e+01,  1.5224e+03],\n",
      "       grad_fn=<RreluWithNoiseBackward0>)\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "(4, tensor(-1.1921e-07, grad_fn=<SqueezeBackward1>))\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "# while True:\n",
    "print(policy.act(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reinforce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/anaconda3/envs/nle/lib/python3.8/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: -9.31\n",
      "Episode 2\tAverage Score: -9.41\n",
      "Episode 3\tAverage Score: -9.45\n",
      "Episode 4\tAverage Score: -8.67\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "scores,gif,last_gif = reinforce(policy, optimizer, n_episodes=5,max_t=1000,print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Video\n",
      "Saved Video\n"
     ]
    }
   ],
   "source": [
    "save_gif(gif)\n",
    "save_gif(last_gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/evan/RL_Assignment/repo')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "pathlib.Path().resolve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nle': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d61d25e1d05acdfa46420ff41b28ca1a60bdfb91fc85449c48770d29c2d77610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
