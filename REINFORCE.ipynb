{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6de4052490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import minihack\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "1. Figure out why its not learning (could be because we are using the whole glyphs, and not just glyphs_crop)\n",
    "1. Figure out how to save and load policy object\n",
    "1. Figure out how to run on a Cluster or on Colab\n",
    "1. Improvements to model/algorithm (maybe just include baseline? Or change to actor-critic)\n",
    "1. Hyperparameter Tuning\n",
    "1. Set up to run with multiple seeds for statistical significance purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful URLs\n",
    "\n",
    "https://minihack.readthedocs.io/en/latest/index.html\n",
    "\n",
    "https://minihack.readthedocs.io/en/latest/getting-started/observation_spaces.html#options\n",
    "\n",
    "https://discuss.pytorch.org/t/pytorch-multiple-input-and-output/140020/2\n",
    "\n",
    "https://goodboychan.github.io/python/reinforcement_learning/pytorch/udacity/2021/05/12/REINFORCE-CartPole.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapes(observation):\n",
    "    '''\n",
    "    Args: \n",
    "        observation: (dict) An environment observation\n",
    "    Ret:\n",
    "        shapes: (list) A list of the shapes of the numpy arrays in the observation dict\n",
    "    '''\n",
    "    shapes = []\n",
    "    for key in observation.keys():\n",
    "        if not key == \"pixel\":\n",
    "            thing = observation[key]\n",
    "            shapes.append(thing.shape)\n",
    "    return shapes\n",
    "\n",
    "def save_gif(gif,path=None):\n",
    "    '''\n",
    "    Args:\n",
    "        gif: a list of image objects\n",
    "        path: the path to save the gif to, defaults to <current datetime>.gif\n",
    "    '''\n",
    "    if path is None:\n",
    "        path = '/home/evan/RL_Assignment/repo/video'+datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") + '.gif'\n",
    "    gif[0].save(path, save_all=True,optimize=False, append_images=gif[1:], loop=0)\n",
    "    print(\"Saved Video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"/home/evan/RL_Assignment/ttyrec_files\"\n",
    "device = torch.device(\"cpu\")\n",
    "# obs_keys = ('glyphs', 'chars','screen_descriptions', 'glyphs_crop', 'chars_crop', 'screen_descriptions_crop','blstats', 'message', 'inv_strs', 'pixel')\n",
    "obs_keys = ('glyphs_crop','inv_glyphs', 'pixel')\n",
    "\n",
    "env = gym.make(\n",
    "    \"MiniHack-Quest-Hard-v0\",\n",
    "    savedir=save_dir,\n",
    "    observation_keys=obs_keys\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, env, hidden_size=100):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        # initialising state\n",
    "        state = env.reset()\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        # Getting the shapes of the state space\n",
    "        shapes = get_shapes(state)\n",
    "        self.obs_keys = obs_keys\n",
    "\n",
    "        input_size = 136 # self.get_input_shape(shapes)\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def get_input_shape(self,shapes):\n",
    "        length = 0\n",
    "        for shape in shapes:\n",
    "            length +=sum(shape)\n",
    "        return length\n",
    "\n",
    "    def forward(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action probabilites\n",
    "\n",
    "        '''\n",
    "        \n",
    "        glyph_features = torch.from_numpy(state['glyphs_crop']).float().to(device)\n",
    "        glyph_features = torch.flatten(glyph_features)\n",
    "        inv_features = torch.from_numpy(state['inv_glyphs']).to(device)\n",
    "        x = torch.cat((glyph_features.float(),inv_features.float()))\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_probabilites = F.relu(self.fc2(x))\n",
    "        # we just consider 1 dimensional probability of action\n",
    "        return F.softmax(action_probabilites, dim=-1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        '''\n",
    "        Args: state, an observation from env.step(). \n",
    "            state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "        Ret: action,probability(action|policy,state)\n",
    "\n",
    "        '''\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Policy(nn.Module):\n",
    "#     def __init__(self, env, hidden_size=100):\n",
    "#         super(Policy, self).__init__()\n",
    "\n",
    "#         # initialising state\n",
    "#         state = env.reset()\n",
    "#         self.glyphState = np.array([\n",
    "#             state['glyphs'],\n",
    "#             state['glyphs'],\n",
    "#             state['glyphs']\n",
    "#         ])\n",
    "#         action_size = env.action_space.n\n",
    "\n",
    "#         # Getting the shapes of the state space\n",
    "#         shapes = get_shapes(state)\n",
    "#         self.obs_keys = obs_keys\n",
    "\n",
    "#         self.glyphConv2D1 = nn.Conv2d(3,1,(3,5),stride=(2,3))\n",
    "#         self.glyphMaxPool1 = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "#         self.fc1 = nn.Linear(115, hidden_size)\n",
    "#         self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         '''\n",
    "#         Args: state, an observation from env.step(). \n",
    "#             state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "#         Ret: action probabilites\n",
    "\n",
    "#         '''\n",
    "#         self.glyphState = np.insert(self.glyphState[:-1,:,:],0,state['glyphs'],axis=0)\n",
    "        \n",
    "#         glyph_features = torch.from_numpy(self.glyphState ).float().unsqueeze(0).to(device)\n",
    "#         glyph_features = F.relu(self.glyphConv2D1(glyph_features))\n",
    "#         glyph_features = torch.flatten(self.glyphMaxPool1(glyph_features))\n",
    "#         inv_features = torch.from_numpy(state['inv_glyphs']).to(device)\n",
    "#         x = torch.cat((glyph_features.float(),inv_features.float()))\n",
    "        \n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         action_probabilites = F.relu(self.fc2(x))\n",
    "#         # we just consider 1 dimensional probability of action\n",
    "#         return F.softmax(action_probabilites, dim=-1)\n",
    "    \n",
    "#     def act(self, state):\n",
    "#         '''\n",
    "#         Args: state, an observation from env.step(). \n",
    "#             state is a dictionary, with keys \"glyphs\" and \"inv_glyphs\"\n",
    "#         Ret: action,probability(action|policy,state)\n",
    "\n",
    "#         '''\n",
    "#         probs = self.forward(state).cpu()\n",
    "#         model = Categorical(probs)\n",
    "#         action = model.sample()\n",
    "#         return action.item(), model.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    '''\n",
    "    Args:\n",
    "        policy: a pytorch model, takes in a state and outputs an action\n",
    "        optimizer: a pytorch optimizer\n",
    "        n_episodes: (int) number of episodes to train for\n",
    "        max_t: (int) max time steps per episode\n",
    "        gamma: (float, [0,1]) discount factor\n",
    "        print_every: (int) number of episodes between print of update\n",
    "    \n",
    "    Ret:\n",
    "        scores: list of total rewards per episode\n",
    "        gif: a list of images corresponding to the best scoring episode\n",
    "    '''\n",
    "    print(\"Starting Reinforce\")\n",
    "    best_frames = None\n",
    "    best_score = None\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(1, n_episodes):\n",
    "        frames = []\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # Collect trajectory\n",
    "        for t in range(max_t):\n",
    "            # Sample the action from current policy\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action) \n",
    "            frame = state[\"pixel\"]\n",
    "            frames.append(frame)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        # Calculate total expected reward\n",
    "        scores_deque.append(sum(rewards))\n",
    "        score = sum(rewards)\n",
    "        scores.append(score)\n",
    "\n",
    "        # Keep track of the gif of the best scoring episode\n",
    "        if best_score is None or score > best_score: \n",
    "            best_score=score\n",
    "            best_frames = frames\n",
    "        \n",
    "        # Recalculate the total reward applying discounted factor\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        R = sum([a * b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        # Calculate the loss \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            # Note that we are using Gradient Ascent, not Descent. So we need to calculate it with negative rewards.\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        # After that, we concatenate whole policy loss in 0th dimension\n",
    "        # print(policy_loss[0])\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if e % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= 195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e - 100, np.mean(scores_deque)))\n",
    "            break\n",
    "\n",
    "    gif = []\n",
    "    for image in best_frames:\n",
    "        gif.append(Image.fromarray(image, \"RGB\"))\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return scores,gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(env).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reinforce\n",
      "Episode 5\tAverage Score: -10.00\n",
      "Episode 10\tAverage Score: -10.00\n",
      "Episode 15\tAverage Score: -10.00\n",
      "Episode 20\tAverage Score: -10.00\n",
      "Episode 25\tAverage Score: -10.00\n",
      "Episode 30\tAverage Score: -10.00\n",
      "Episode 35\tAverage Score: -10.00\n",
      "Episode 40\tAverage Score: -10.00\n",
      "Episode 45\tAverage Score: -10.00\n",
      "Episode 50\tAverage Score: -10.00\n",
      "Episode 55\tAverage Score: -10.00\n",
      "Episode 60\tAverage Score: -10.00\n",
      "Episode 65\tAverage Score: -10.00\n",
      "Episode 70\tAverage Score: -10.00\n",
      "Episode 75\tAverage Score: -10.00\n",
      "Episode 80\tAverage Score: -10.00\n",
      "Episode 85\tAverage Score: -10.00\n",
      "Episode 90\tAverage Score: -10.00\n",
      "Episode 95\tAverage Score: -10.00\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "scores,gif = reinforce(policy, optimizer, n_episodes=100,max_t=1000,print_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Video\n"
     ]
    }
   ],
   "source": [
    "save_gif(gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/evan/RL_Assignment/repo')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "pathlib.Path().resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nle': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d61d25e1d05acdfa46420ff41b28ca1a60bdfb91fc85449c48770d29c2d77610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
